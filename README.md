# Multi-Armed-Bandits
This repository contains the presentation materials from my talk on multi-armed bandits, reinforcement learning and the bayesian vs frequentist view.
Two algorithms covered as part of the presentation include Upper Confidence Bound and Thompson Sampling.
The presentation also includes theory on beta distribution, posterior and priors, likelihood functions and conjugate priors
